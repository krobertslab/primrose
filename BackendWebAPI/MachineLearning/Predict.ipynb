{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Masking, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vly2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vly2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import requests \n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "def parsegene(xmlfile): \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xmlfile) \n",
    "    # get root element \n",
    "    root = tree.getroot()\n",
    "    # create empty list for news items \n",
    "    geneitems = [] \n",
    "    # iterate news items \n",
    "    for item in root.findall('./topic/gene'): \n",
    "        gene = item.text\n",
    "        # append news dictionary to news items list \n",
    "        geneitems.append(gene)  \n",
    "    # return news items list \n",
    "    for i in range(19,21):\n",
    "        geneitems.remove(geneitems[i])\n",
    "    genes = str(geneitems)\n",
    "    genes = re.sub(\"[a-x]|\\[|\\]|\\'|\\,|\\(|\\)|\\W\\d+\\W|[y].\",\"\", genes).split(\" \")\n",
    "    geneslist = []                \n",
    "    for i in genes:\n",
    "        if i != \"\":\n",
    "            geneslist.append(i)\n",
    "    pattern = \"|\".join(geneslist)\n",
    "    return pattern\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    genes = parsegene(\"Topics/topics2018.xml\")\n",
    "    ## Remove puncuation\n",
    "    text = str(text)\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"\\[\\D\\S{6}:\\S{5}\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"\\\\n*\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r'\\,|\\.|\\:|\\&', \"\", text)\n",
    "    text = re.sub(r'\\{|\\}|\\[|\\]|\\(|\\)', \"\", text)\n",
    "    text = re.sub(genes, \"gene\", text)\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "vocabulary_size = 20000\n",
    "def process(df):\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "    tokenizer.fit_on_texts(df['Abstracts'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['Abstracts'])\n",
    "    data = pad_sequences(sequences, maxlen=500)\n",
    "    df_save = pd.DataFrame(data)\n",
    "    return df_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>194</td>\n",
       "      <td>127</td>\n",
       "      <td>88</td>\n",
       "      <td>170</td>\n",
       "      <td>1544</td>\n",
       "      <td>1821</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "      <td>389</td>\n",
       "      <td>222</td>\n",
       "      <td>972</td>\n",
       "      <td>404</td>\n",
       "      <td>53</td>\n",
       "      <td>389</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>1132</td>\n",
       "      <td>1162</td>\n",
       "      <td>483</td>\n",
       "      <td>112</td>\n",
       "      <td>162</td>\n",
       "      <td>133</td>\n",
       "      <td>263</td>\n",
       "      <td>2281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>385</td>\n",
       "      <td>183</td>\n",
       "      <td>107</td>\n",
       "      <td>644</td>\n",
       "      <td>477</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>536</td>\n",
       "      <td>220</td>\n",
       "      <td>134</td>\n",
       "      <td>88</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  490  491   492  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...   13   48    10   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    4   60    53   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    2   59  1132   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    3   10   385   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...  107    3    13   \n",
       "\n",
       "    493  494  495  496   497   498   499  \n",
       "0   194  127   88  170  1544  1821    28  \n",
       "1   389  222  972  404    53   389   415  \n",
       "2  1162  483  112  162   133   263  2281  \n",
       "3   183  107  644  477    85     1     2  \n",
       "4    48   58  536  220   134    88   399  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's ndcg@1: 0.346667\tvalid_0's ndcg@3: 0.358287\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's ndcg@1: 0.586667\tvalid_0's ndcg@3: 0.54946\n",
      "[3]\tvalid_0's ndcg@1: 0.666667\tvalid_0's ndcg@3: 0.664841\n",
      "[4]\tvalid_0's ndcg@1: 0.713333\tvalid_0's ndcg@3: 0.698436\n",
      "[5]\tvalid_0's ndcg@1: 0.873333\tvalid_0's ndcg@3: 0.751956\n",
      "[6]\tvalid_0's ndcg@1: 0.853333\tvalid_0's ndcg@3: 0.763874\n",
      "[7]\tvalid_0's ndcg@1: 0.86\tvalid_0's ndcg@3: 0.790466\n",
      "[8]\tvalid_0's ndcg@1: 0.893333\tvalid_0's ndcg@3: 0.801564\n",
      "[9]\tvalid_0's ndcg@1: 0.866667\tvalid_0's ndcg@3: 0.814153\n",
      "[10]\tvalid_0's ndcg@1: 0.906667\tvalid_0's ndcg@3: 0.817691\n",
      "[11]\tvalid_0's ndcg@1: 0.88\tvalid_0's ndcg@3: 0.814301\n",
      "[12]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.821229\n",
      "[13]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.824357\n",
      "[14]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.828715\n",
      "[15]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.832588\n",
      "[16]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.830205\n",
      "[17]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.831843\n",
      "[18]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.832253\n",
      "[19]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.82946\n",
      "[20]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.836871\n",
      "[21]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.827076\n",
      "[22]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.825512\n",
      "[23]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.822383\n",
      "[24]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[25]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.831769\n",
      "[26]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[27]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[28]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[29]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[30]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.831769\n",
      "[31]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.823948\n",
      "[32]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.827076\n",
      "[33]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.827076\n",
      "[34]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.825512\n",
      "[35]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.824283\n",
      "[36]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.823464\n",
      "[37]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.826592\n",
      "[38]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.828902\n",
      "[39]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.828902\n",
      "[40]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.83095\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.836871\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "x_train, y_train = sklearn.datasets.load_svmlight_file(\"data/train.txt\")\n",
    "x_valid, y_valid = sklearn.datasets.load_svmlight_file(\"data/val.txt\")\n",
    "q_train = np.loadtxt('data/train.group')\n",
    "q_valid = np.loadtxt('data/val.group')\n",
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRanker()\n",
    "bst = gbm.fit(x_train, y_train, group=q_train, eval_set=[(x_valid, y_valid)],\n",
    "eval_group=[q_valid], eval_at=[1, 3], early_stopping_rounds=20, verbose=True,\n",
    "callbacks=[lgb.reset_parameter(learning_rate=lambda x: 0.95 ** x * 0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    data = pd.read_json(path_or_buf = data).iloc[:,3:4].rename(columns={\"_source\":\"Abstracts\"})\n",
    "    df = data.dropna()\n",
    "    df['Abstracts'] = df['Abstracts'].map(lambda x: clean_text(x))\n",
    "    test_x = process(df)\n",
    "    preds = gbm.predict(test_x)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15110153, -0.06419284, -0.21651752, ..., -0.14455386,\n",
       "       -0.05767295,  0.02914337])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'data/supafile.txt'\n",
    "prediction = predict(data)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
