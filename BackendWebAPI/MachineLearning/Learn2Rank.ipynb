{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Masking, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(\"Learn2rank/datal2r/abstracts2018.csv\", encoding='latin-1').iloc[:,1:]\n",
    "id_data = pd.read_csv(\"Learn2rank/datal2r/abstracts.judgments.2018.csv\", encoding='latin-1')\n",
    "data = pd.merge(id_data, content, how='right', left_on=\"trec_doc_id\", right_on=\"ID\").iloc[:, [0, -2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-982a1483fd96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#data.head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"trec_topic_number\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#data.head()\n",
    "data = data.rename(columns={\"trec_topic_number\": \"ID\"}).sort_values(by=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-fc50e9c0f893>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import requests \n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "def parsegene(xmlfile): \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xmlfile) \n",
    "    # get root element \n",
    "    root = tree.getroot()\n",
    "    # create empty list for news items \n",
    "    geneitems = [] \n",
    "    # iterate news items \n",
    "    for item in root.findall('./topic/gene'): \n",
    "        gene = item.text\n",
    "        # append news dictionary to news items list \n",
    "        geneitems.append(gene)  \n",
    "    # return news items list \n",
    "    for i in range(19,21):\n",
    "        geneitems.remove(geneitems[i])\n",
    "    genes = str(geneitems)\n",
    "    genes = re.sub(\"[a-x]|\\[|\\]|\\'|\\,|\\(|\\)|\\W\\d+\\W|[y].\",\"\", genes).split(\" \")\n",
    "    geneslist = []                \n",
    "    for i in genes:\n",
    "        if i != \"\":\n",
    "            geneslist.append(i)\n",
    "    pattern = \"|\".join(geneslist)\n",
    "    return pattern\n",
    "\n",
    "df = data.dropna()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    genes = parsegene(\"Topics/topics2018.xml\")\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"\\[\\D\\S{6}:\\S{5}\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"\\\\n*\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r'\\,|\\.|\\:|\\&', \"\", text)\n",
    "    text = re.sub(r'\\{|\\}|\\[|\\]|\\(|\\)', \"\", text)\n",
    "    text = re.sub(genes, \"gene\", text)\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "df['Abstracts'] = df['Abstracts'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000\n",
    "def process(df):\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "    tokenizer.fit_on_texts(df['Abstracts'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['Abstracts'])\n",
    "    data = pad_sequences(sequences, maxlen=500)\n",
    "    labels = np.array(df[\"Class\"])\n",
    "    df_save = pd.DataFrame(data)\n",
    "    df_save.head(10)\n",
    "    df_label = pd.DataFrame(np.array(labels))\n",
    "    print(df.head())\n",
    "    df_id = pd.DataFrame(np.array(df[\"ID\"]))\n",
    "    datamat = pd.concat([df_save, df_label], axis = 1, ignore_index = True)\n",
    "    datamat = pd.concat([df_id, datamat], axis = 1, ignore_index = True)\n",
    "    return data, labels, datamat, tokenizer, labels\n",
    "x, y, vecdf, tokenizer, labels = process(df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(vecdf, test_size=0.2)\n",
    "train_x = train.iloc[:,1:-1]\n",
    "train_y = train.iloc[:,-1]\n",
    "train_id = train.iloc[:,0]\n",
    "test_x = test.iloc[:,1:-1]\n",
    "test_y = test.iloc[:,-1]\n",
    "test_id = test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19314</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>463</td>\n",
       "      <td>86</td>\n",
       "      <td>173</td>\n",
       "      <td>24</td>\n",
       "      <td>463</td>\n",
       "      <td>506</td>\n",
       "      <td>8</td>\n",
       "      <td>1549</td>\n",
       "      <td>957</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>77</td>\n",
       "      <td>31</td>\n",
       "      <td>1565</td>\n",
       "      <td>5109</td>\n",
       "      <td>1024</td>\n",
       "      <td>2332</td>\n",
       "      <td>817</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>362</td>\n",
       "      <td>69</td>\n",
       "      <td>6</td>\n",
       "      <td>613</td>\n",
       "      <td>86</td>\n",
       "      <td>480</td>\n",
       "      <td>707</td>\n",
       "      <td>821</td>\n",
       "      <td>3294</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>344</td>\n",
       "      <td>189</td>\n",
       "      <td>34</td>\n",
       "      <td>504</td>\n",
       "      <td>304</td>\n",
       "      <td>4576</td>\n",
       "      <td>682</td>\n",
       "      <td>949</td>\n",
       "      <td>55</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3    4    5    6    7    8    9    10   ...  491  492  493  \\\n",
       "19314    0    0    0    0    0    0    0    0    0    0  ...  463   86  173   \n",
       "4860     0    0    0    0    0    0    0    0    0    0  ...    8   77   31   \n",
       "14160    0    0    0    0    0    0    0    0    0    0  ...  362   69    6   \n",
       "4119     0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "20169    0    0    0    0    0    0    0    0    0    0  ...  344  189   34   \n",
       "\n",
       "        494   495   496   497   498   499   500  \n",
       "19314    24   463   506     8  1549   957   149  \n",
       "4860   1565  5109  1024  2332   817   119     1  \n",
       "14160   613    86   480   707   821  3294     4  \n",
       "4119      0     0     0     0     0     0  1031  \n",
       "20169   504   304  4576   682   949    55   476  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "x_train, y_train = sklearn.datasets.load_svmlight_file(\"data/train.txt\")\n",
    "x_valid, y_valid = sklearn.datasets.load_svmlight_file(\"data/val.txt\")\n",
    "x_test, y_test = sklearn.datasets.load_svmlight_file(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_train = np.loadtxt('data/train.group')\n",
    "q_valid = np.loadtxt('data/val.group')\n",
    "q_test = np.loadtxt('data/test.group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's ndcg@1: 0.346667\tvalid_0's ndcg@3: 0.358287\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's ndcg@1: 0.586667\tvalid_0's ndcg@3: 0.54946\n",
      "[3]\tvalid_0's ndcg@1: 0.666667\tvalid_0's ndcg@3: 0.664841\n",
      "[4]\tvalid_0's ndcg@1: 0.713333\tvalid_0's ndcg@3: 0.698436\n",
      "[5]\tvalid_0's ndcg@1: 0.873333\tvalid_0's ndcg@3: 0.751956\n",
      "[6]\tvalid_0's ndcg@1: 0.853333\tvalid_0's ndcg@3: 0.763874\n",
      "[7]\tvalid_0's ndcg@1: 0.86\tvalid_0's ndcg@3: 0.790466\n",
      "[8]\tvalid_0's ndcg@1: 0.893333\tvalid_0's ndcg@3: 0.801564\n",
      "[9]\tvalid_0's ndcg@1: 0.866667\tvalid_0's ndcg@3: 0.814153\n",
      "[10]\tvalid_0's ndcg@1: 0.906667\tvalid_0's ndcg@3: 0.817691\n",
      "[11]\tvalid_0's ndcg@1: 0.88\tvalid_0's ndcg@3: 0.814301\n",
      "[12]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.821229\n",
      "[13]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.824357\n",
      "[14]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.828715\n",
      "[15]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.832588\n",
      "[16]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.830205\n",
      "[17]\tvalid_0's ndcg@1: 0.886667\tvalid_0's ndcg@3: 0.831843\n",
      "[18]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.832253\n",
      "[19]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.82946\n",
      "[20]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.836871\n",
      "[21]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.827076\n",
      "[22]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.825512\n",
      "[23]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.822383\n",
      "[24]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[25]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.831769\n",
      "[26]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[27]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[28]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[29]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.832588\n",
      "[30]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.831769\n",
      "[31]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.823948\n",
      "[32]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.827076\n",
      "[33]\tvalid_0's ndcg@1: 0.9\tvalid_0's ndcg@3: 0.827076\n",
      "[34]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.825512\n",
      "[35]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.824283\n",
      "[36]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.823464\n",
      "[37]\tvalid_0's ndcg@1: 0.926667\tvalid_0's ndcg@3: 0.826592\n",
      "[38]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.828902\n",
      "[39]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.828902\n",
      "[40]\tvalid_0's ndcg@1: 0.94\tvalid_0's ndcg@3: 0.83095\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's ndcg@1: 0.913333\tvalid_0's ndcg@3: 0.836871\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRanker()\n",
    "bst = gbm.fit(x_train, y_train, group=q_train, eval_set=[(x_valid, y_valid)],\n",
    "eval_group=[q_valid], eval_at=[1, 3], early_stopping_rounds=20, verbose=True,\n",
    "callbacks=[lgb.reset_parameter(learning_rate=lambda x: 0.95 ** x * 0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.3908338387085487, pvalue=0.0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "preds_train = gbm.predict(x_train)\n",
    "preds_train\n",
    "spearmanr(y_train, preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17223,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.3112367809291524, pvalue=3.0967241053496516e-121)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = gbm.predict(x_test)\n",
    "preds_test\n",
    "spearmanr(y_test, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5383,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
